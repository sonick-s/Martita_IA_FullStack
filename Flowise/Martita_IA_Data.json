{
  "AgentFlow": [],
  "AgentFlowV2": [
    {
      "id": "3a06d0f3-5627-4d62-9d55-bc0cf2e34797",
      "name": "Martita_Agente",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"startAgentflow_0\",\n      \"type\": \"agentFlow\",\n      \"position\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"data\": {\n        \"id\": \"startAgentflow_0\",\n        \"label\": \"Start\",\n        \"version\": 1.1,\n        \"name\": \"startAgentflow\",\n        \"type\": \"Start\",\n        \"color\": \"#7EE787\",\n        \"hideInput\": true,\n        \"baseClasses\": [\n          \"Start\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Starting point of the agentflow\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Type\",\n            \"name\": \"startInputType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Chat Input\",\n                \"name\": \"chatInput\",\n                \"description\": \"Start the conversation with chat input\"\n              },\n              {\n                \"label\": \"Form Input\",\n                \"name\": \"formInput\",\n                \"description\": \"Start the workflow with form inputs\"\n              }\n            ],\n            \"default\": \"chatInput\",\n            \"id\": \"startAgentflow_0-input-startInputType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Form Title\",\n            \"name\": \"formTitle\",\n            \"type\": \"string\",\n            \"placeholder\": \"Please Fill Out The Form\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formTitle-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Description\",\n            \"name\": \"formDescription\",\n            \"type\": \"string\",\n            \"placeholder\": \"Complete all fields below to continue\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formDescription-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Input Types\",\n            \"name\": \"formInputTypes\",\n            \"description\": \"Specify the type of form input\",\n            \"type\": \"array\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"array\": [\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Options\",\n                    \"name\": \"options\"\n                  }\n                ],\n                \"default\": \"string\"\n              },\n              {\n                \"label\": \"Label\",\n                \"name\": \"label\",\n                \"type\": \"string\",\n                \"placeholder\": \"Label for the input\"\n              },\n              {\n                \"label\": \"Variable Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"placeholder\": \"Variable name for the input (must be camel case)\",\n                \"description\": \"Variable name must be camel case. For example: firstName, lastName, etc.\"\n              },\n              {\n                \"label\": \"Add Options\",\n                \"name\": \"addOptions\",\n                \"type\": \"array\",\n                \"show\": {\n                  \"formInputTypes[$index].type\": \"options\"\n                },\n                \"array\": [\n                  {\n                    \"label\": \"Option\",\n                    \"name\": \"option\",\n                    \"type\": \"string\"\n                  }\n                ]\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-formInputTypes-array\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Ephemeral Memory\",\n            \"name\": \"startEphemeralMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Start fresh for every execution without past chat history\",\n            \"optional\": true\n          },\n          {\n            \"label\": \"Flow State\",\n            \"name\": \"startState\",\n            \"description\": \"Runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\",\n                \"placeholder\": \"Foo\"\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"placeholder\": \"Bar\"\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-startState-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Persist State\",\n            \"name\": \"startPersistState\",\n            \"type\": \"boolean\",\n            \"description\": \"Persist the state in the same session\",\n            \"optional\": true,\n            \"id\": \"startAgentflow_0-input-startPersistState-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"startInputType\": \"chatInput\",\n          \"formTitle\": \"\",\n          \"formDescription\": \"\",\n          \"formInputTypes\": \"\",\n          \"startState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"\"\n            }\n          ]\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"startAgentflow_0-output-startAgentflow\",\n            \"label\": \"Start\",\n            \"name\": \"startAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 139,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_0\",\n      \"position\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_0\",\n        \"label\": \"Check if query valid\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"AI Related\"\n              },\n              {\n                \"scenario\": \"General\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatGoogleGenerativeAI\",\n          \"conditionAgentInstructions\": \"<p>Check if user is asking about AI related topic, or just general query</p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"AI Related\"\n            },\n            {\n              \"scenario\": \"General\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"conditionAgentModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_0\",\n      \"position\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_0\",\n        \"label\": \"Generate Query\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_0-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_0-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_0-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p><strong>Example</strong>:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p></p><p><strong>Example</strong>:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p></p><p>Question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>Query:</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span></p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_0-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_1\",\n      \"position\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_1\",\n        \"label\": \"General Answer\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_1-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_1-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_1-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_1-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverAgentflow_0\",\n      \"position\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"data\": {\n        \"id\": \"retrieverAgentflow_0\",\n        \"label\": \"Retriever Vector DB\",\n        \"version\": 1,\n        \"name\": \"retrieverAgentflow\",\n        \"type\": \"Retriever\",\n        \"color\": \"#b8bedd\",\n        \"baseClasses\": [\n          \"Retriever\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Retrieve information from vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Knowledge (Document Stores)\",\n            \"name\": \"retrieverKnowledgeDocumentStores\",\n            \"type\": \"array\",\n            \"description\": \"Document stores to retrieve information from. Document stores must be upserted in advance.\",\n            \"array\": [\n              {\n                \"label\": \"Document Store\",\n                \"name\": \"documentStore\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listStores\"\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Retriever Query\",\n            \"name\": \"retrieverQuery\",\n            \"type\": \"string\",\n            \"placeholder\": \"Enter your query here\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"id\": \"retrieverAgentflow_0-input-retrieverQuery-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Output Format\",\n            \"name\": \"outputFormat\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Text\",\n                \"name\": \"text\"\n              },\n              {\n                \"label\": \"Text with Metadata\",\n                \"name\": \"textWithMetadata\"\n              }\n            ],\n            \"default\": \"text\",\n            \"id\": \"retrieverAgentflow_0-input-outputFormat-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"retrieverUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"retrieverKnowledgeDocumentStores\": [\n            {\n              \"documentStore\": \"570df92b-087b-4d3b-9462-7a11283454a5:ai paper\"\n            }\n          ],\n          \"retrieverQuery\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p>\",\n          \"outputFormat\": \"text\",\n          \"retrieverUpdateState\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n            \"label\": \"Retriever\",\n            \"name\": \"retrieverAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 325,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_1\",\n      \"position\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_1\",\n        \"label\": \"Check if docs relevant\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"Relevant\"\n              },\n              {\n                \"scenario\": \"Irrelevant\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Determine if the document is relevant to user question. User question is <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"Relevant\"\n            },\n            {\n              \"scenario\": \"Irrelevant\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 365,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_2\",\n      \"position\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_2\",\n        \"label\": \"Generate Response\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_2-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_2-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_2-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": \"\",\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Given the question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>And the findings: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response</p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_2-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 192,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_3\",\n      \"position\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_3\",\n        \"label\": \"Regenerate Question\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_3-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_3-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_3-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>You are a helpful assistant that can transform the query to produce a better question.</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question:</p><p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p><p>Formulate an improved question:</p><p></p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span> </p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_3-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 209,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"loopAgentflow_0\",\n      \"position\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"data\": {\n        \"id\": \"loopAgentflow_0\",\n        \"label\": \"Loop back to Retriever\",\n        \"version\": 1,\n        \"name\": \"loopAgentflow\",\n        \"type\": \"Loop\",\n        \"color\": \"#FFA07A\",\n        \"hideOutput\": true,\n        \"baseClasses\": [\n          \"Loop\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Loop back to a previous node\",\n        \"inputParams\": [\n          {\n            \"label\": \"Loop Back To\",\n            \"name\": \"loopBackToNode\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listPreviousNodes\",\n            \"freeSolo\": true,\n            \"id\": \"loopAgentflow_0-input-loopBackToNode-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Loop Count\",\n            \"name\": \"maxLoopCount\",\n            \"type\": \"number\",\n            \"default\": 5,\n            \"id\": \"loopAgentflow_0-input-maxLoopCount-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"loopBackToNode\": \"retrieverAgentflow_0-Retriever Vector DB\",\n          \"maxLoopCount\": 5\n        },\n        \"outputAnchors\": [],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 207,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_0\",\n      \"position\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_0\",\n        \"label\": \"Sticky Note\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_0-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"First update of the state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_0-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_1\",\n      \"position\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_1\",\n        \"label\": \"Sticky Note (1)\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_1-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"Second update of state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_1-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-0\",\n      \"target\": \"llmAgentflow_0\",\n      \"targetHandle\": \"llmAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-1\",\n      \"target\": \"llmAgentflow_1\",\n      \"targetHandle\": \"llmAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1\"\n    },\n    {\n      \"source\": \"startAgentflow_0\",\n      \"sourceHandle\": \"startAgentflow_0-output-startAgentflow\",\n      \"target\": \"conditionAgentAgentflow_0\",\n      \"targetHandle\": \"conditionAgentAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#7EE787\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0\"\n    },\n    {\n      \"source\": \"llmAgentflow_0\",\n      \"sourceHandle\": \"llmAgentflow_0-output-llmAgentflow\",\n      \"target\": \"retrieverAgentflow_0\",\n      \"targetHandle\": \"retrieverAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#b8bedd\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0\"\n    },\n    {\n      \"source\": \"retrieverAgentflow_0\",\n      \"sourceHandle\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n      \"target\": \"conditionAgentAgentflow_1\",\n      \"targetHandle\": \"conditionAgentAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#b8bedd\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1\"\n    },\n    {\n      \"source\": \"llmAgentflow_3\",\n      \"sourceHandle\": \"llmAgentflow_3-output-llmAgentflow\",\n      \"target\": \"loopAgentflow_0\",\n      \"targetHandle\": \"loopAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#FFA07A\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_3-llmAgentflow_3-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-1\",\n      \"target\": \"llmAgentflow_3\",\n      \"targetHandle\": \"llmAgentflow_3\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_3-llmAgentflow_3\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-0\",\n      \"target\": \"llmAgentflow_2\",\n      \"targetHandle\": \"llmAgentflow_2\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2\"\n    }\n  ]\n}",
      "type": "AGENTFLOW"
    },
    {
      "id": "97f4182e-dd06-498f-973e-046360e1c563",
      "name": "Martita_Agente",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"startAgentflow_0\",\n      \"type\": \"agentFlow\",\n      \"position\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"data\": {\n        \"id\": \"startAgentflow_0\",\n        \"label\": \"Start\",\n        \"version\": 1.1,\n        \"name\": \"startAgentflow\",\n        \"type\": \"Start\",\n        \"color\": \"#7EE787\",\n        \"hideInput\": true,\n        \"baseClasses\": [\n          \"Start\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Starting point of the agentflow\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Type\",\n            \"name\": \"startInputType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Chat Input\",\n                \"name\": \"chatInput\",\n                \"description\": \"Start the conversation with chat input\"\n              },\n              {\n                \"label\": \"Form Input\",\n                \"name\": \"formInput\",\n                \"description\": \"Start the workflow with form inputs\"\n              }\n            ],\n            \"default\": \"chatInput\",\n            \"id\": \"startAgentflow_0-input-startInputType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Form Title\",\n            \"name\": \"formTitle\",\n            \"type\": \"string\",\n            \"placeholder\": \"Please Fill Out The Form\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formTitle-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Description\",\n            \"name\": \"formDescription\",\n            \"type\": \"string\",\n            \"placeholder\": \"Complete all fields below to continue\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formDescription-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Input Types\",\n            \"name\": \"formInputTypes\",\n            \"description\": \"Specify the type of form input\",\n            \"type\": \"array\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"array\": [\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Options\",\n                    \"name\": \"options\"\n                  }\n                ],\n                \"default\": \"string\"\n              },\n              {\n                \"label\": \"Label\",\n                \"name\": \"label\",\n                \"type\": \"string\",\n                \"placeholder\": \"Label for the input\"\n              },\n              {\n                \"label\": \"Variable Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"placeholder\": \"Variable name for the input (must be camel case)\",\n                \"description\": \"Variable name must be camel case. For example: firstName, lastName, etc.\"\n              },\n              {\n                \"label\": \"Add Options\",\n                \"name\": \"addOptions\",\n                \"type\": \"array\",\n                \"show\": {\n                  \"formInputTypes[$index].type\": \"options\"\n                },\n                \"array\": [\n                  {\n                    \"label\": \"Option\",\n                    \"name\": \"option\",\n                    \"type\": \"string\"\n                  }\n                ]\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-formInputTypes-array\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Ephemeral Memory\",\n            \"name\": \"startEphemeralMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Start fresh for every execution without past chat history\",\n            \"optional\": true\n          },\n          {\n            \"label\": \"Flow State\",\n            \"name\": \"startState\",\n            \"description\": \"Runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\",\n                \"placeholder\": \"Foo\"\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"placeholder\": \"Bar\"\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-startState-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Persist State\",\n            \"name\": \"startPersistState\",\n            \"type\": \"boolean\",\n            \"description\": \"Persist the state in the same session\",\n            \"optional\": true,\n            \"id\": \"startAgentflow_0-input-startPersistState-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"startInputType\": \"chatInput\",\n          \"formTitle\": \"\",\n          \"formDescription\": \"\",\n          \"formInputTypes\": \"\",\n          \"startState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"\"\n            }\n          ]\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"startAgentflow_0-output-startAgentflow\",\n            \"label\": \"Start\",\n            \"name\": \"startAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 139,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_0\",\n      \"position\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_0\",\n        \"label\": \"Check if query valid\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"AI Related\"\n              },\n              {\n                \"scenario\": \"General\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatGoogleGenerativeAI\",\n          \"conditionAgentInstructions\": \"<p>Check if user is asking about AI related topic, or just general query</p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"AI Related\"\n            },\n            {\n              \"scenario\": \"General\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"conditionAgentModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_0\",\n      \"position\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_0\",\n        \"label\": \"Generate Query\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_0-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_0-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_0-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p><strong>Example</strong>:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p></p><p><strong>Example</strong>:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p></p><p>Question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>Query:</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span></p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_0-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_1\",\n      \"position\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_1\",\n        \"label\": \"General Answer\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_1-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_1-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_1-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_1-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverAgentflow_0\",\n      \"position\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"data\": {\n        \"id\": \"retrieverAgentflow_0\",\n        \"label\": \"Retriever Vector DB\",\n        \"version\": 1,\n        \"name\": \"retrieverAgentflow\",\n        \"type\": \"Retriever\",\n        \"color\": \"#b8bedd\",\n        \"baseClasses\": [\n          \"Retriever\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Retrieve information from vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Knowledge (Document Stores)\",\n            \"name\": \"retrieverKnowledgeDocumentStores\",\n            \"type\": \"array\",\n            \"description\": \"Document stores to retrieve information from. Document stores must be upserted in advance.\",\n            \"array\": [\n              {\n                \"label\": \"Document Store\",\n                \"name\": \"documentStore\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listStores\"\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Retriever Query\",\n            \"name\": \"retrieverQuery\",\n            \"type\": \"string\",\n            \"placeholder\": \"Enter your query here\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"id\": \"retrieverAgentflow_0-input-retrieverQuery-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Output Format\",\n            \"name\": \"outputFormat\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Text\",\n                \"name\": \"text\"\n              },\n              {\n                \"label\": \"Text with Metadata\",\n                \"name\": \"textWithMetadata\"\n              }\n            ],\n            \"default\": \"text\",\n            \"id\": \"retrieverAgentflow_0-input-outputFormat-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"retrieverUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"retrieverKnowledgeDocumentStores\": [\n            {\n              \"documentStore\": \"570df92b-087b-4d3b-9462-7a11283454a5:ai paper\"\n            }\n          ],\n          \"retrieverQuery\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p>\",\n          \"outputFormat\": \"text\",\n          \"retrieverUpdateState\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n            \"label\": \"Retriever\",\n            \"name\": \"retrieverAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 325,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_1\",\n      \"position\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_1\",\n        \"label\": \"Check if docs relevant\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"Relevant\"\n              },\n              {\n                \"scenario\": \"Irrelevant\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Determine if the document is relevant to user question. User question is <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"Relevant\"\n            },\n            {\n              \"scenario\": \"Irrelevant\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 365,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_2\",\n      \"position\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_2\",\n        \"label\": \"Generate Response\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_2-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_2-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_2-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": \"\",\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Given the question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>And the findings: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response</p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_2-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 192,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_3\",\n      \"position\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_3\",\n        \"label\": \"Regenerate Question\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_3-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_3-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_3-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>You are a helpful assistant that can transform the query to produce a better question.</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question:</p><p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p><p>Formulate an improved question:</p><p></p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span> </p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_3-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 209,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"loopAgentflow_0\",\n      \"position\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"data\": {\n        \"id\": \"loopAgentflow_0\",\n        \"label\": \"Loop back to Retriever\",\n        \"version\": 1,\n        \"name\": \"loopAgentflow\",\n        \"type\": \"Loop\",\n        \"color\": \"#FFA07A\",\n        \"hideOutput\": true,\n        \"baseClasses\": [\n          \"Loop\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Loop back to a previous node\",\n        \"inputParams\": [\n          {\n            \"label\": \"Loop Back To\",\n            \"name\": \"loopBackToNode\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listPreviousNodes\",\n            \"freeSolo\": true,\n            \"id\": \"loopAgentflow_0-input-loopBackToNode-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Loop Count\",\n            \"name\": \"maxLoopCount\",\n            \"type\": \"number\",\n            \"default\": 5,\n            \"id\": \"loopAgentflow_0-input-maxLoopCount-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"loopBackToNode\": \"retrieverAgentflow_0-Retriever Vector DB\",\n          \"maxLoopCount\": 5\n        },\n        \"outputAnchors\": [],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 207,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_0\",\n      \"position\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_0\",\n        \"label\": \"Sticky Note\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_0-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"First update of the state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_0-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_1\",\n      \"position\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_1\",\n        \"label\": \"Sticky Note (1)\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_1-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"Second update of state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_1-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-0\",\n      \"target\": \"llmAgentflow_0\",\n      \"targetHandle\": \"llmAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-1\",\n      \"target\": \"llmAgentflow_1\",\n      \"targetHandle\": \"llmAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1\"\n    },\n    {\n      \"source\": \"startAgentflow_0\",\n      \"sourceHandle\": \"startAgentflow_0-output-startAgentflow\",\n      \"target\": \"conditionAgentAgentflow_0\",\n      \"targetHandle\": \"conditionAgentAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#7EE787\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0\"\n    },\n    {\n      \"source\": \"llmAgentflow_0\",\n      \"sourceHandle\": \"llmAgentflow_0-output-llmAgentflow\",\n      \"target\": \"retrieverAgentflow_0\",\n      \"targetHandle\": \"retrieverAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#b8bedd\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0\"\n    },\n    {\n      \"source\": \"retrieverAgentflow_0\",\n      \"sourceHandle\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n      \"target\": \"conditionAgentAgentflow_1\",\n      \"targetHandle\": \"conditionAgentAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#b8bedd\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1\"\n    },\n    {\n      \"source\": \"llmAgentflow_3\",\n      \"sourceHandle\": \"llmAgentflow_3-output-llmAgentflow\",\n      \"target\": \"loopAgentflow_0\",\n      \"targetHandle\": \"loopAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#FFA07A\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_3-llmAgentflow_3-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-1\",\n      \"target\": \"llmAgentflow_3\",\n      \"targetHandle\": \"llmAgentflow_3\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_3-llmAgentflow_3\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-0\",\n      \"target\": \"llmAgentflow_2\",\n      \"targetHandle\": \"llmAgentflow_2\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2\"\n    }\n  ]\n}",
      "type": "AGENTFLOW"
    }
  ],
  "AssistantFlow": [],
  "AssistantCustom": [],
  "AssistantOpenAI": [],
  "AssistantAzure": [],
  "ChatFlow": [
    {
      "id": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "name": "Martita_IA_Flow",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"apiLoader_0\",\n      \"position\": {\n        \"x\": -166.343619787935,\n        \"y\": 399.89435338862205\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"apiLoader_0\",\n        \"label\": \"API Loader\",\n        \"version\": 2.1,\n        \"name\": \"apiLoader\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from an API\",\n        \"inputParams\": [\n          {\n            \"label\": \"Method\",\n            \"name\": \"method\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"GET\",\n                \"name\": \"GET\"\n              },\n              {\n                \"label\": \"POST\",\n                \"name\": \"POST\"\n              }\n            ],\n            \"id\": \"apiLoader_0-input-method-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"URL\",\n            \"name\": \"url\",\n            \"type\": \"string\",\n            \"id\": \"apiLoader_0-input-url-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Headers\",\n            \"name\": \"headers\",\n            \"type\": \"json\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-headers-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"SSL Certificate\",\n            \"description\": \"Please upload a SSL certificate file in either .pem or .crt\",\n            \"name\": \"caFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pem, .crt\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-caFile-file\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Body\",\n            \"name\": \"body\",\n            \"type\": \"json\",\n            \"description\": \"JSON body for the POST request. If not specified, agent will try to figure out itself from AIPlugin if provided\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-body-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"apiLoader_0-input-metadata-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"apiLoader_0-input-omitMetadataKeys-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-textSplitter-TextSplitter\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"\",\n          \"method\": \"GET\",\n          \"url\": \"http://backend:8000/construir-tramite/estructurado\",\n          \"headers\": \"\",\n          \"body\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"apiLoader_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"apiLoader_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -166.343619787935,\n        \"y\": 399.89435338862205\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"googleGenerativeAiEmbeddings_0\",\n      \"position\": {\n        \"x\": -177.07919178623737,\n        \"y\": 1054.4384799357392\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"googleGenerativeAiEmbeddings_0\",\n        \"label\": \"GoogleGenerativeAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"googleGenerativeAiEmbeddings\",\n        \"type\": \"GoogleGenerativeAiEmbeddings\",\n        \"baseClasses\": [\n          \"GoogleGenerativeAiEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Google Generative API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embedding-001\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Task Type\",\n            \"name\": \"tasktype\",\n            \"type\": \"options\",\n            \"description\": \"Type of task for which the embedding will be used\",\n            \"options\": [\n              {\n                \"label\": \"TASK_TYPE_UNSPECIFIED\",\n                \"name\": \"TASK_TYPE_UNSPECIFIED\"\n              },\n              {\n                \"label\": \"RETRIEVAL_QUERY\",\n                \"name\": \"RETRIEVAL_QUERY\"\n              },\n              {\n                \"label\": \"RETRIEVAL_DOCUMENT\",\n                \"name\": \"RETRIEVAL_DOCUMENT\"\n              },\n              {\n                \"label\": \"SEMANTIC_SIMILARITY\",\n                \"name\": \"SEMANTIC_SIMILARITY\"\n              },\n              {\n                \"label\": \"CLASSIFICATION\",\n                \"name\": \"CLASSIFICATION\"\n              },\n              {\n                \"label\": \"CLUSTERING\",\n                \"name\": \"CLUSTERING\"\n              }\n            ],\n            \"default\": \"TASK_TYPE_UNSPECIFIED\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-tasktype-options\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"embedding-001\",\n          \"tasktype\": \"TASK_TYPE_UNSPECIFIED\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n            \"name\": \"googleGenerativeAiEmbeddings\",\n            \"label\": \"GoogleGenerativeAiEmbeddings\",\n            \"description\": \"Google Generative API to generate embeddings for a given text\",\n            \"type\": \"GoogleGenerativeAiEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 472,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -177.07919178623737,\n        \"y\": 1054.4384799357392\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 552.7455578583165,\n        \"y\": 725.7816352501283\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{apiLoader_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{googleGenerativeAiEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/root/.flowise/embeddings/\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 464,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 552.7455578583165,\n        \"y\": 725.7816352501283\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 1063.2360426708017,\n        \"y\": -80.25597234853285\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3.1,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Safety Settings\",\n            \"name\": \"safetySettings\",\n            \"type\": \"array\",\n            \"description\": \"Safety settings for the model. Refer to the <a href=\\\"https://ai.google.dev/gemini-api/docs/safety-settings\\\">official guide</a> on how to use Safety Settings\",\n            \"array\": [\n              {\n                \"label\": \"Harm Category\",\n                \"name\": \"harmCategory\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"Dangerous\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Promotes, facilitates, or encourages harmful acts.\"\n                  },\n                  {\n                    \"label\": \"Harassment\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Negative or harmful comments targeting identity and/or protected attributes.\"\n                  },\n                  {\n                    \"label\": \"Hate Speech\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Content that is rude, disrespectful, or profane.\"\n                  },\n                  {\n                    \"label\": \"Sexually Explicit\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Contains references to sexual acts or other lewd content.\"\n                  },\n                  {\n                    \"label\": \"Civic Integrity\",\n                    \"name\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n                    \"description\": \"Election-related queries.\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Harm Block Threshold\",\n                \"name\": \"harmBlockThreshold\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"None\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"Always show regardless of probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Only High\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Block when high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Medium and Above\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Block when medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Low and Above\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Block when low, medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Threshold Unspecified (Default Threshold)\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified, block using default threshold\"\n                  }\n                ]\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-safetySettings-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"description\": \"Base URL for the API. Leave empty to use the default.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-baseUrl-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"{{inMemoryCache_0.data.instance}}\",\n          \"modelName\": \"gemini-2.0-flash\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.2\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"safetySettings\": \"\",\n          \"baseUrl\": \"\",\n          \"allowImageUploads\": false\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 675,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1063.2360426708017,\n        \"y\": -80.25597234853285\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmFilterRetriever_0\",\n      \"position\": {\n        \"x\": 1071.503095056429,\n        \"y\": 820.8527376848419\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmFilterRetriever_0\",\n        \"label\": \"LLM Filter Retriever\",\n        \"version\": 1,\n        \"name\": \"llmFilterRetriever\",\n        \"type\": \"LLMFilterRetriever\",\n        \"baseClasses\": [\n          \"LLMFilterRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Retrievers\",\n        \"description\": \"Iterate over the initially returned documents and extract, from each, only the content that is relevant to the query\",\n        \"inputParams\": [\n          {\n            \"label\": \"Query\",\n            \"name\": \"query\",\n            \"type\": \"string\",\n            \"description\": \"Query to retrieve documents from retriever. If not specified, user question will be used\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"id\": \"llmFilterRetriever_0-input-query-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"baseRetriever\",\n            \"type\": \"VectorStoreRetriever\",\n            \"id\": \"llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmFilterRetriever_0-input-model-BaseLanguageModel\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"baseRetriever\": \"{{faiss_0.data.instance}}\",\n          \"model\": \"{{chatGoogleGenerativeAI_1.data.instance}}\",\n          \"query\": \"{x}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"LLM Filter Retriever\",\n                \"description\": \"\",\n                \"type\": \"LLMFilterRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"llmFilterRetriever_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"llmFilterRetriever_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 413,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1071.503095056429,\n        \"y\": 820.8527376848419\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1671.1273723742863,\n        \"y\": 795.9535459456622\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{llmFilterRetriever_0.data.instance}}\",\n          \"memory\": \"\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"INSTRUCCIONES:\\n\\nPrimer mensaje:\\nIdentifica el trmite que ms se aproxime a la consulta del usuario y presntalo. Pregunta si desea ms informacin sobre cmo realizar el proceso.\\n\\nSegundo mensaje:\\nProporciona ms detalles sobre el trmite y los requisitos necesarios para llevarlo a cabo.\\n\\nTercer mensaje:\\nExplica los pasos a seguir para completar el trmite. Incluye los enlaces a los formularios u otros documentos necesarios.\\n\\nMensaje final:\\nPregunta si desea ayuda con otro trmite. Agradece su consulta y sugirele que deje un \\\"like\\\" o una \\\"manito arriba\\\" si la informacin fue til.\\n\\nHistorial del Chat:\\n{chat_history}\\n\\nPregunta de Seguimiento:{question}\\n\\nPregunta Independiente:\",\n          \"responsePrompt\": \"Eres Martita AI, una asesora tcnica municipal del GAD de Cayambe. \\n\\n\\n\\nREGLAS INQUEBRANTABLES:\\n- NUNCA inventes informacin que no est en el contexto\\n- Siempre responde en espaol\\n- Mantn un tono profesional pero amigable\\n- Si hay dudas, deriva al contacto humano correspondiente\\n\\nContexto Proporcionado:\\n------------\\n{context}\\n------------\\n\\nPregunta del Usuario: {question}\\nINSTRUCCIONES:\\n\\nPrimer mensaje:\\nIdentifica el trmite que ms se aproxime a la consulta del usuario y presntalo. Pregunta si desea ms informacin sobre cmo realizar el proceso.\\n\\nSegundo mensaje:\\nProporciona ms detalles sobre el trmite y los requisitos necesarios para llevarlo a cabo.\\n\\nTercer mensaje:\\nExplica los pasos a seguir para completar el trmite. Incluye los enlaces a los formularios u otros documentos necesarios.\\n\\nMensaje final:\\nPregunta si desea ayuda con otro trmite. Agradece su consulta y sugirele que deje un \\\"like\\\" o una \\\"manito arriba\\\" si la informacin fue til.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 536,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1671.1273723742863,\n        \"y\": 795.9535459456622\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_1\",\n      \"position\": {\n        \"x\": 569.4321312826156,\n        \"y\": 1342.7945064455434\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_1\",\n        \"label\": \"ChatGoogleGenerativeAI (1)\",\n        \"version\": 3.1,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_1-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_1-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-customModelName-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-temperature-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-streaming-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-maxOutputTokens-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-topP-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-topK-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Safety Settings\",\n            \"name\": \"safetySettings\",\n            \"type\": \"array\",\n            \"description\": \"Safety settings for the model. Refer to the <a href=\\\"https://ai.google.dev/gemini-api/docs/safety-settings\\\">official guide</a> on how to use Safety Settings\",\n            \"array\": [\n              {\n                \"label\": \"Harm Category\",\n                \"name\": \"harmCategory\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"Dangerous\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Promotes, facilitates, or encourages harmful acts.\"\n                  },\n                  {\n                    \"label\": \"Harassment\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Negative or harmful comments targeting identity and/or protected attributes.\"\n                  },\n                  {\n                    \"label\": \"Hate Speech\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Content that is rude, disrespectful, or profane.\"\n                  },\n                  {\n                    \"label\": \"Sexually Explicit\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Contains references to sexual acts or other lewd content.\"\n                  },\n                  {\n                    \"label\": \"Civic Integrity\",\n                    \"name\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n                    \"description\": \"Election-related queries.\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Harm Block Threshold\",\n                \"name\": \"harmBlockThreshold\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"None\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"Always show regardless of probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Only High\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Block when high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Medium and Above\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Block when medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Low and Above\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Block when low, medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Threshold Unspecified (Default Threshold)\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified, block using default threshold\"\n                  }\n                ]\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-safetySettings-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"description\": \"Base URL for the API. Leave empty to use the default.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-baseUrl-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-allowImageUploads-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-cache-BaseCache\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.1\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"safetySettings\": \"\",\n          \"baseUrl\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 675,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 569.4321312826156,\n        \"y\": 1342.7945064455434\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"inMemoryCache_0\",\n      \"position\": {\n        \"x\": 559.1182205060782,\n        \"y\": -2.9027537712468927\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"inMemoryCache_0\",\n        \"label\": \"InMemory Cache\",\n        \"version\": 1,\n        \"name\": \"inMemoryCache\",\n        \"type\": \"InMemoryCache\",\n        \"baseClasses\": [\n          \"InMemoryCache\",\n          \"BaseCache\"\n        ],\n        \"category\": \"Cache\",\n        \"description\": \"Cache LLM response in memory, will be cleared once app restarted\",\n        \"inputParams\": [],\n        \"inputAnchors\": [],\n        \"inputs\": {},\n        \"outputAnchors\": [\n          {\n            \"id\": \"inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache\",\n            \"name\": \"inMemoryCache\",\n            \"label\": \"InMemoryCache\",\n            \"description\": \"Cache LLM response in memory, will be cleared once app restarted\",\n            \"type\": \"InMemoryCache | BaseCache\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 149,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 559.1182205060782,\n        \"y\": -2.9027537712468927\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"llmFilterRetriever_0\",\n      \"sourceHandle\": \"llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"llmFilterRetriever_0-llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"llmFilterRetriever_0\",\n      \"targetHandle\": \"llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-llmFilterRetriever_0-llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\"\n    },\n    {\n      \"source\": \"chatGoogleGenerativeAI_1\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmFilterRetriever_0\",\n      \"targetHandle\": \"llmFilterRetriever_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_1-chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-llmFilterRetriever_0-llmFilterRetriever_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"apiLoader_0\",\n      \"sourceHandle\": \"apiLoader_0-output-document-Document|json\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"apiLoader_0-apiLoader_0-output-document-Document|json-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"googleGenerativeAiEmbeddings_0\",\n      \"sourceHandle\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"googleGenerativeAiEmbeddings_0-googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"inMemoryCache_0\",\n      \"sourceHandle\": \"inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache\",\n      \"target\": \"chatGoogleGenerativeAI_0\",\n      \"targetHandle\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\",\n      \"type\": \"buttonedge\",\n      \"id\": \"inMemoryCache_0-inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache-chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    }
  ],
  "ChatMessage": [
    {
      "id": "ad1b4277-4e89-433b-93e8-14c08359cada",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "hola",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:50:42.557Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "2768bb66-5677-46c8-af3b-2282cd82a38d",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "Hola!  Soy Martita AI, tu asesora tcnica municipal del GAD de Cayambe.\n\nParece que necesitas informacin sobre algn trmite. Para ayudarte mejor, podras decirme qu tipo de trmite te interesa realizar? As podr darte la informacin precisa que necesitas. \n",
      "sourceDocuments": "[]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:50:46.372Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "61437893-9177-4787-9031-c0843aba84de",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "necesito cambiar el propietario de un terreno , como puedo hacer el tramite",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:43.853Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "d5e892b9-4ba4-4226-8201-9620eb565be9",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "Entiendo! El trmite que necesitas es la **Transferencia de Dominio de un bien inmueble**. Te gustara que te explique cmo realizar este proceso aqu en Cayambe? \n",
      "sourceDocuments": "[]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:49.162Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "6f6bf4ce-faa7-46ed-bb04-f8907b1c5e65",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "si ",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:59.092Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "d0427f5f-ab18-4497-9002-59f017e07437",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "Perfecto! Para realizar la **Emisin de Avalo para Transferencia de Dominio Total** en Cayambe, necesitars los siguientes requisitos:\n\n1.  Original de la cdula de ciudadana del vendedor y comprador.\n2.  Copia de la escritura del terreno.\n3.  Pago del impuesto predial actualizado.\n4.  Certificado de gravmenes actualizado.\n5.  Formulario de Trmites de Avalos y Catastros debidamente llenado.\n6.  Certificado de plusvala (solo si el terreno es urbano).\n\nTienes alguna pregunta sobre estos requisitos?\n",
      "sourceDocuments": "[{\"pageContent\":\"- \\\"nombre\\\": \\\"EMISIN DE AVALO PARA TRANSFERENCIA DE DOMINIO TOTAL\\\",\\n- \\\"descripcion\\\": \\\"Determinar y emitir el avalo para la transferencia total de\\\\ndominio de los predios.\\\",\\n- \\\"contexto\\\": \\\"Determinar y emitir el avalo para la transferencia total de dominio de los predios.\\\",\\n- \\\"requisitos\\\": [\\n      {\\n        \\\"contexto\\\": \\\"Cualquier persona\\\",\\n        \\\"requisito\\\": \\\"1. Original de cdula de ciudadana de vendedor y\\\\ncomprador.\\\\n2. Copia de la escritura.\\\\n3. Pago del impuesto predial actualizado.\\\\n4. Certificado de gravmenes actualizado.\\\\n5. Formulario de Trmites de Avalos y Catastros.\\\\n6. Certificado de plusvala (slo urbano).\\\",\\n      }\\n    ],\\n- \\\"formularios\\\": [\\n      {\\n        \\\"url\\\": \\\"https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf\\\",\\n        \\\"contexto\\\": \\\"FORMULARIO DE TRMITES DE AVALOS Y CATASTROS\\\",\\n      }\\n    ],\\n- \\\"pasos\\\": [\\n      {\\n        \\\"contexto\\\": \\\"Cualquier persona\\\",\\n        \\\"paso\\\": \\\"1. Imprimir y llenar el Formulario de Trmites de Avalos y\\\\nCatastros que est disponible en la pgina web del GADIPMC. El formulario tambin puede ser completado desde el\\\\ncomputador para luego imprimirse.\\\\n2. Adquirir certificado de plusvala en caja del GADIP-MC.\\\\n3. Ingresar requisitos en la Secretara de la Direccin de\\\\nAvalos y Catastros.\\\",\\n      }\\n    ],\\n- \\\"nombre\\\": \\\"Direccin de Avalos y Catastros\\\",\\n- \\\"descripcion\\\": \\\"Encargada del levantamiento catastral, valoracin de predios urbanos y rurales, actualizacin de la base catastral, emisin de certificados de avalos y mantenimiento del sistema de informacin territorial\\\",\",\"metadata\":{\"url\":\"http://backend:8000/construir-tramite/estructurado\"}}]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:52:09.399Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "11da8d2e-cc31-421e-839c-27ac37a99e5f",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "cuentame mas",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:53:26.257Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "c69a17a6-e014-4b87-bb5b-e70ba56fd7aa",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "Claro! Aqu estn los pasos a seguir para completar el trmite:\n\n1.  **Descarga y llena el Formulario de Trmites de Avalos y Catastros:** Puedes encontrarlo en la pgina web del GADIPMC: [https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf](https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf). Puedes completar el formulario directamente en tu computadora antes de imprimirlo si lo prefieres.\n2.  **Adquiere el certificado de plusvala:** Si tu terreno es urbano, debes obtener este certificado en las cajas del GADIP-MC.\n3.  **Entrega los requisitos:** Una vez que tengas todos los documentos y el formulario lleno, debes entregarlos en la Secretara de la Direccin de Avalos y Catastros.\n\nY eso es todo! Te puedo ayudar con algo ms? \n",
      "sourceDocuments": "[{\"pageContent\":\"- `nombre\\\": \\\"EMISIN DE AVALO PARA TRANSFERENCIA DE DOMINIO TOTAL`\\n- `descripcion\\\": \\\"Determinar y emitir el avalo para la transferencia total de\\\\ndominio de los predios.`\\n- `requisito\\\": \\\"1. Original de cdula de ciudadana de vendedor y\\\\ncomprador.\\\\n2. Copia de la escritura.\\\\n3. Pago del impuesto predial actualizado.\\\\n4. Certificado de gravmenes actualizado.\\\\n5. Formulario de Trmites de Avalos y Catastros.\\\\n6. Certificado de plusvala (slo urbano).`\\n- `url\\\": \\\"https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf`\\n- `contexto\\\": \\\"FORMULARIO DE TRMITES DE AVALOS Y CATASTROS`\\n- `paso\\\": \\\"1. Imprimir y llenar el Formulario de Trmites de Avalos y\\\\nCatastros que est disponible en la pgina web del GADIPMC. El formulario tambin puede ser completado desde el\\\\ncomputador para luego imprimirse.\\\\n2. Adquirir certificado de plusvala en caja del GADIP-MC.\\\\n3. Ingresar requisitos en la Secretara de la Direccin de\\\\nAvalos y Catastros.`\\n- `nombre\\\": \\\"Direccin de Avalos y Catastros`\",\"metadata\":{\"url\":\"http://backend:8000/construir-tramite/estructurado\"}}]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:53:35.132Z",
      "leadEmail": null,
      "followUpPrompts": null
    }
  ],
  "ChatMessageFeedback": [],
  "CustomTemplate": [],
  "DocumentStore": [],
  "DocumentStoreFileChunk": [],
  "Execution": [],
  "Tool": [],
  "Variable": []
}