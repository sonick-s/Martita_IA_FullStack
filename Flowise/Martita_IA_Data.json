{
  "AgentFlow": [],
  "AgentFlowV2": [
    {
      "id": "3a06d0f3-5627-4d62-9d55-bc0cf2e34797",
      "name": "Martita_Agente",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"startAgentflow_0\",\n      \"type\": \"agentFlow\",\n      \"position\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"data\": {\n        \"id\": \"startAgentflow_0\",\n        \"label\": \"Start\",\n        \"version\": 1.1,\n        \"name\": \"startAgentflow\",\n        \"type\": \"Start\",\n        \"color\": \"#7EE787\",\n        \"hideInput\": true,\n        \"baseClasses\": [\n          \"Start\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Starting point of the agentflow\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Type\",\n            \"name\": \"startInputType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Chat Input\",\n                \"name\": \"chatInput\",\n                \"description\": \"Start the conversation with chat input\"\n              },\n              {\n                \"label\": \"Form Input\",\n                \"name\": \"formInput\",\n                \"description\": \"Start the workflow with form inputs\"\n              }\n            ],\n            \"default\": \"chatInput\",\n            \"id\": \"startAgentflow_0-input-startInputType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Form Title\",\n            \"name\": \"formTitle\",\n            \"type\": \"string\",\n            \"placeholder\": \"Please Fill Out The Form\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formTitle-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Description\",\n            \"name\": \"formDescription\",\n            \"type\": \"string\",\n            \"placeholder\": \"Complete all fields below to continue\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formDescription-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Input Types\",\n            \"name\": \"formInputTypes\",\n            \"description\": \"Specify the type of form input\",\n            \"type\": \"array\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"array\": [\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Options\",\n                    \"name\": \"options\"\n                  }\n                ],\n                \"default\": \"string\"\n              },\n              {\n                \"label\": \"Label\",\n                \"name\": \"label\",\n                \"type\": \"string\",\n                \"placeholder\": \"Label for the input\"\n              },\n              {\n                \"label\": \"Variable Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"placeholder\": \"Variable name for the input (must be camel case)\",\n                \"description\": \"Variable name must be camel case. For example: firstName, lastName, etc.\"\n              },\n              {\n                \"label\": \"Add Options\",\n                \"name\": \"addOptions\",\n                \"type\": \"array\",\n                \"show\": {\n                  \"formInputTypes[$index].type\": \"options\"\n                },\n                \"array\": [\n                  {\n                    \"label\": \"Option\",\n                    \"name\": \"option\",\n                    \"type\": \"string\"\n                  }\n                ]\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-formInputTypes-array\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Ephemeral Memory\",\n            \"name\": \"startEphemeralMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Start fresh for every execution without past chat history\",\n            \"optional\": true\n          },\n          {\n            \"label\": \"Flow State\",\n            \"name\": \"startState\",\n            \"description\": \"Runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\",\n                \"placeholder\": \"Foo\"\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"placeholder\": \"Bar\"\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-startState-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Persist State\",\n            \"name\": \"startPersistState\",\n            \"type\": \"boolean\",\n            \"description\": \"Persist the state in the same session\",\n            \"optional\": true,\n            \"id\": \"startAgentflow_0-input-startPersistState-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"startInputType\": \"chatInput\",\n          \"formTitle\": \"\",\n          \"formDescription\": \"\",\n          \"formInputTypes\": \"\",\n          \"startState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"\"\n            }\n          ]\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"startAgentflow_0-output-startAgentflow\",\n            \"label\": \"Start\",\n            \"name\": \"startAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 139,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_0\",\n      \"position\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_0\",\n        \"label\": \"Check if query valid\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"AI Related\"\n              },\n              {\n                \"scenario\": \"General\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatGoogleGenerativeAI\",\n          \"conditionAgentInstructions\": \"<p>Check if user is asking about AI related topic, or just general query</p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"AI Related\"\n            },\n            {\n              \"scenario\": \"General\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"conditionAgentModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_0\",\n      \"position\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_0\",\n        \"label\": \"Generate Query\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_0-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_0-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_0-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p><strong>Example</strong>:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p></p><p><strong>Example</strong>:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p></p><p>Question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>Query:</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span></p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_0-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_1\",\n      \"position\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_1\",\n        \"label\": \"General Answer\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_1-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_1-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_1-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_1-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverAgentflow_0\",\n      \"position\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"data\": {\n        \"id\": \"retrieverAgentflow_0\",\n        \"label\": \"Retriever Vector DB\",\n        \"version\": 1,\n        \"name\": \"retrieverAgentflow\",\n        \"type\": \"Retriever\",\n        \"color\": \"#b8bedd\",\n        \"baseClasses\": [\n          \"Retriever\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Retrieve information from vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Knowledge (Document Stores)\",\n            \"name\": \"retrieverKnowledgeDocumentStores\",\n            \"type\": \"array\",\n            \"description\": \"Document stores to retrieve information from. Document stores must be upserted in advance.\",\n            \"array\": [\n              {\n                \"label\": \"Document Store\",\n                \"name\": \"documentStore\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listStores\"\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Retriever Query\",\n            \"name\": \"retrieverQuery\",\n            \"type\": \"string\",\n            \"placeholder\": \"Enter your query here\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"id\": \"retrieverAgentflow_0-input-retrieverQuery-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Output Format\",\n            \"name\": \"outputFormat\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Text\",\n                \"name\": \"text\"\n              },\n              {\n                \"label\": \"Text with Metadata\",\n                \"name\": \"textWithMetadata\"\n              }\n            ],\n            \"default\": \"text\",\n            \"id\": \"retrieverAgentflow_0-input-outputFormat-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"retrieverUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"retrieverKnowledgeDocumentStores\": [\n            {\n              \"documentStore\": \"570df92b-087b-4d3b-9462-7a11283454a5:ai paper\"\n            }\n          ],\n          \"retrieverQuery\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p>\",\n          \"outputFormat\": \"text\",\n          \"retrieverUpdateState\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n            \"label\": \"Retriever\",\n            \"name\": \"retrieverAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 325,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_1\",\n      \"position\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_1\",\n        \"label\": \"Check if docs relevant\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"Relevant\"\n              },\n              {\n                \"scenario\": \"Irrelevant\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Determine if the document is relevant to user question. User question is <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"Relevant\"\n            },\n            {\n              \"scenario\": \"Irrelevant\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 365,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_2\",\n      \"position\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_2\",\n        \"label\": \"Generate Response\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_2-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_2-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_2-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": \"\",\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Given the question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>And the findings: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response</p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_2-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 192,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_3\",\n      \"position\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_3\",\n        \"label\": \"Regenerate Question\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_3-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_3-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_3-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>You are a helpful assistant that can transform the query to produce a better question.</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question:</p><p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p><p>Formulate an improved question:</p><p></p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span> </p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_3-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 209,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"loopAgentflow_0\",\n      \"position\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"data\": {\n        \"id\": \"loopAgentflow_0\",\n        \"label\": \"Loop back to Retriever\",\n        \"version\": 1,\n        \"name\": \"loopAgentflow\",\n        \"type\": \"Loop\",\n        \"color\": \"#FFA07A\",\n        \"hideOutput\": true,\n        \"baseClasses\": [\n          \"Loop\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Loop back to a previous node\",\n        \"inputParams\": [\n          {\n            \"label\": \"Loop Back To\",\n            \"name\": \"loopBackToNode\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listPreviousNodes\",\n            \"freeSolo\": true,\n            \"id\": \"loopAgentflow_0-input-loopBackToNode-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Loop Count\",\n            \"name\": \"maxLoopCount\",\n            \"type\": \"number\",\n            \"default\": 5,\n            \"id\": \"loopAgentflow_0-input-maxLoopCount-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"loopBackToNode\": \"retrieverAgentflow_0-Retriever Vector DB\",\n          \"maxLoopCount\": 5\n        },\n        \"outputAnchors\": [],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 207,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_0\",\n      \"position\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_0\",\n        \"label\": \"Sticky Note\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_0-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"First update of the state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_0-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_1\",\n      \"position\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_1\",\n        \"label\": \"Sticky Note (1)\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_1-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"Second update of state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_1-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-0\",\n      \"target\": \"llmAgentflow_0\",\n      \"targetHandle\": \"llmAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-1\",\n      \"target\": \"llmAgentflow_1\",\n      \"targetHandle\": \"llmAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1\"\n    },\n    {\n      \"source\": \"startAgentflow_0\",\n      \"sourceHandle\": \"startAgentflow_0-output-startAgentflow\",\n      \"target\": \"conditionAgentAgentflow_0\",\n      \"targetHandle\": \"conditionAgentAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#7EE787\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0\"\n    },\n    {\n      \"source\": \"llmAgentflow_0\",\n      \"sourceHandle\": \"llmAgentflow_0-output-llmAgentflow\",\n      \"target\": \"retrieverAgentflow_0\",\n      \"targetHandle\": \"retrieverAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#b8bedd\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0\"\n    },\n    {\n      \"source\": \"retrieverAgentflow_0\",\n      \"sourceHandle\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n      \"target\": \"conditionAgentAgentflow_1\",\n      \"targetHandle\": \"conditionAgentAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#b8bedd\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1\"\n    },\n    {\n      \"source\": \"llmAgentflow_3\",\n      \"sourceHandle\": \"llmAgentflow_3-output-llmAgentflow\",\n      \"target\": \"loopAgentflow_0\",\n      \"targetHandle\": \"loopAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#FFA07A\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_3-llmAgentflow_3-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-1\",\n      \"target\": \"llmAgentflow_3\",\n      \"targetHandle\": \"llmAgentflow_3\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_3-llmAgentflow_3\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-0\",\n      \"target\": \"llmAgentflow_2\",\n      \"targetHandle\": \"llmAgentflow_2\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2\"\n    }\n  ]\n}",
      "type": "AGENTFLOW"
    },
    {
      "id": "97f4182e-dd06-498f-973e-046360e1c563",
      "name": "Martita_Agente",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"startAgentflow_0\",\n      \"type\": \"agentFlow\",\n      \"position\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"data\": {\n        \"id\": \"startAgentflow_0\",\n        \"label\": \"Start\",\n        \"version\": 1.1,\n        \"name\": \"startAgentflow\",\n        \"type\": \"Start\",\n        \"color\": \"#7EE787\",\n        \"hideInput\": true,\n        \"baseClasses\": [\n          \"Start\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Starting point of the agentflow\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Type\",\n            \"name\": \"startInputType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Chat Input\",\n                \"name\": \"chatInput\",\n                \"description\": \"Start the conversation with chat input\"\n              },\n              {\n                \"label\": \"Form Input\",\n                \"name\": \"formInput\",\n                \"description\": \"Start the workflow with form inputs\"\n              }\n            ],\n            \"default\": \"chatInput\",\n            \"id\": \"startAgentflow_0-input-startInputType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Form Title\",\n            \"name\": \"formTitle\",\n            \"type\": \"string\",\n            \"placeholder\": \"Please Fill Out The Form\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formTitle-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Description\",\n            \"name\": \"formDescription\",\n            \"type\": \"string\",\n            \"placeholder\": \"Complete all fields below to continue\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formDescription-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Input Types\",\n            \"name\": \"formInputTypes\",\n            \"description\": \"Specify the type of form input\",\n            \"type\": \"array\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"array\": [\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Options\",\n                    \"name\": \"options\"\n                  }\n                ],\n                \"default\": \"string\"\n              },\n              {\n                \"label\": \"Label\",\n                \"name\": \"label\",\n                \"type\": \"string\",\n                \"placeholder\": \"Label for the input\"\n              },\n              {\n                \"label\": \"Variable Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"placeholder\": \"Variable name for the input (must be camel case)\",\n                \"description\": \"Variable name must be camel case. For example: firstName, lastName, etc.\"\n              },\n              {\n                \"label\": \"Add Options\",\n                \"name\": \"addOptions\",\n                \"type\": \"array\",\n                \"show\": {\n                  \"formInputTypes[$index].type\": \"options\"\n                },\n                \"array\": [\n                  {\n                    \"label\": \"Option\",\n                    \"name\": \"option\",\n                    \"type\": \"string\"\n                  }\n                ]\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-formInputTypes-array\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Ephemeral Memory\",\n            \"name\": \"startEphemeralMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Start fresh for every execution without past chat history\",\n            \"optional\": true\n          },\n          {\n            \"label\": \"Flow State\",\n            \"name\": \"startState\",\n            \"description\": \"Runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\",\n                \"placeholder\": \"Foo\"\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"placeholder\": \"Bar\"\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-startState-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Persist State\",\n            \"name\": \"startPersistState\",\n            \"type\": \"boolean\",\n            \"description\": \"Persist the state in the same session\",\n            \"optional\": true,\n            \"id\": \"startAgentflow_0-input-startPersistState-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"startInputType\": \"chatInput\",\n          \"formTitle\": \"\",\n          \"formDescription\": \"\",\n          \"formInputTypes\": \"\",\n          \"startState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"\"\n            }\n          ]\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"startAgentflow_0-output-startAgentflow\",\n            \"label\": \"Start\",\n            \"name\": \"startAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 139,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -261.54516755177303,\n        \"y\": 62.39402454297252\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_0\",\n      \"position\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_0\",\n        \"label\": \"Check if query valid\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"AI Related\"\n              },\n              {\n                \"scenario\": \"General\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatGoogleGenerativeAI\",\n          \"conditionAgentInstructions\": \"<p>Check if user is asking about AI related topic, or just general query</p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"AI Related\"\n            },\n            {\n              \"scenario\": \"General\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"conditionAgentModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -114.84790789259606,\n        \"y\": 53.22583468442305\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_0\",\n      \"position\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_0\",\n        \"label\": \"Generate Query\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_0-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_0-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_0-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p><strong>Example</strong>:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p></p><p><strong>Example</strong>:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p></p><p>Question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>Query:</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span></p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_0-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 158.29022963739308,\n        \"y\": -20.666608318859062\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_1\",\n      \"position\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_1\",\n        \"label\": \"General Answer\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_1-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_1-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_1-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatGoogleGenerativeAI\",\n          \"llmMessages\": [],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gemini-2.5-pro-preview-03-25\",\n            \"customModelName\": \"gemini-2.5-flash-preview-04-17\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxOutputTokens\": \"\",\n            \"topP\": \"\",\n            \"topK\": \"\",\n            \"safetySettings\": \"\",\n            \"baseUrl\": \"\",\n            \"allowImageUploads\": true,\n            \"llmModel\": \"chatGoogleGenerativeAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_1-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 310,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 165.82871786911647,\n        \"y\": 92.15131805222342\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverAgentflow_0\",\n      \"position\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"data\": {\n        \"id\": \"retrieverAgentflow_0\",\n        \"label\": \"Retriever Vector DB\",\n        \"version\": 1,\n        \"name\": \"retrieverAgentflow\",\n        \"type\": \"Retriever\",\n        \"color\": \"#b8bedd\",\n        \"baseClasses\": [\n          \"Retriever\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Retrieve information from vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Knowledge (Document Stores)\",\n            \"name\": \"retrieverKnowledgeDocumentStores\",\n            \"type\": \"array\",\n            \"description\": \"Document stores to retrieve information from. Document stores must be upserted in advance.\",\n            \"array\": [\n              {\n                \"label\": \"Document Store\",\n                \"name\": \"documentStore\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listStores\"\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Retriever Query\",\n            \"name\": \"retrieverQuery\",\n            \"type\": \"string\",\n            \"placeholder\": \"Enter your query here\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"id\": \"retrieverAgentflow_0-input-retrieverQuery-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Output Format\",\n            \"name\": \"outputFormat\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Text\",\n                \"name\": \"text\"\n              },\n              {\n                \"label\": \"Text with Metadata\",\n                \"name\": \"textWithMetadata\"\n              }\n            ],\n            \"default\": \"text\",\n            \"id\": \"retrieverAgentflow_0-input-outputFormat-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"retrieverUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"retrieverKnowledgeDocumentStores\": [\n            {\n              \"documentStore\": \"570df92b-087b-4d3b-9462-7a11283454a5:ai paper\"\n            }\n          ],\n          \"retrieverQuery\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p>\",\n          \"outputFormat\": \"text\",\n          \"retrieverUpdateState\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n            \"label\": \"Retriever\",\n            \"name\": \"retrieverAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 325,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 396.87575963946966,\n        \"y\": -17.41189617164227\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_1\",\n      \"position\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_1\",\n        \"label\": \"Check if docs relevant\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"Relevant\"\n              },\n              {\n                \"scenario\": \"Irrelevant\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Determine if the document is relevant to user question. User question is <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"Relevant\"\n            },\n            {\n              \"scenario\": \"Irrelevant\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 365,\n      \"height\": 80,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 647.9586712853835,\n        \"y\": -24.93225611691784\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_2\",\n      \"position\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_2\",\n        \"label\": \"Generate Response\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_2-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_2-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_2-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": \"\",\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Given the question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>And the findings: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response</p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_2-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 192,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 920.5416793343077,\n        \"y\": -75.82606372993476\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_3\",\n      \"position\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_3\",\n        \"label\": \"Regenerate Question\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_3-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_3-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_3-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>You are a helpful assistant that can transform the query to produce a better question.</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question:</p><p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p><p>Formulate an improved question:</p><p></p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span> </p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"medium\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_3-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 209,\n      \"height\": 72,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 921.1014768144131,\n        \"y\": 26.898902739007895\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"loopAgentflow_0\",\n      \"position\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"data\": {\n        \"id\": \"loopAgentflow_0\",\n        \"label\": \"Loop back to Retriever\",\n        \"version\": 1,\n        \"name\": \"loopAgentflow\",\n        \"type\": \"Loop\",\n        \"color\": \"#FFA07A\",\n        \"hideOutput\": true,\n        \"baseClasses\": [\n          \"Loop\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Loop back to a previous node\",\n        \"inputParams\": [\n          {\n            \"label\": \"Loop Back To\",\n            \"name\": \"loopBackToNode\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listPreviousNodes\",\n            \"freeSolo\": true,\n            \"id\": \"loopAgentflow_0-input-loopBackToNode-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Loop Count\",\n            \"name\": \"maxLoopCount\",\n            \"type\": \"number\",\n            \"default\": 5,\n            \"id\": \"loopAgentflow_0-input-maxLoopCount-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"loopBackToNode\": \"retrieverAgentflow_0-Retriever Vector DB\",\n          \"maxLoopCount\": 5\n        },\n        \"outputAnchors\": [],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 207,\n      \"height\": 66,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1160.0553838519766,\n        \"y\": 30.06685001229809\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_0\",\n      \"position\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_0\",\n        \"label\": \"Sticky Note\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_0-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"First update of the state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_0-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_1\",\n      \"position\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_1\",\n        \"label\": \"Sticky Note (1)\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_1-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"Second update of state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_1-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-0\",\n      \"target\": \"llmAgentflow_0\",\n      \"targetHandle\": \"llmAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-1\",\n      \"target\": \"llmAgentflow_1\",\n      \"targetHandle\": \"llmAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1\"\n    },\n    {\n      \"source\": \"startAgentflow_0\",\n      \"sourceHandle\": \"startAgentflow_0-output-startAgentflow\",\n      \"target\": \"conditionAgentAgentflow_0\",\n      \"targetHandle\": \"conditionAgentAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#7EE787\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0\"\n    },\n    {\n      \"source\": \"llmAgentflow_0\",\n      \"sourceHandle\": \"llmAgentflow_0-output-llmAgentflow\",\n      \"target\": \"retrieverAgentflow_0\",\n      \"targetHandle\": \"retrieverAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#b8bedd\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0\"\n    },\n    {\n      \"source\": \"retrieverAgentflow_0\",\n      \"sourceHandle\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n      \"target\": \"conditionAgentAgentflow_1\",\n      \"targetHandle\": \"conditionAgentAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#b8bedd\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1\"\n    },\n    {\n      \"source\": \"llmAgentflow_3\",\n      \"sourceHandle\": \"llmAgentflow_3-output-llmAgentflow\",\n      \"target\": \"loopAgentflow_0\",\n      \"targetHandle\": \"loopAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#FFA07A\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"llmAgentflow_3-llmAgentflow_3-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-1\",\n      \"target\": \"llmAgentflow_3\",\n      \"targetHandle\": \"llmAgentflow_3\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_3-llmAgentflow_3\"\n    },\n    {\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-0\",\n      \"target\": \"llmAgentflow_2\",\n      \"targetHandle\": \"llmAgentflow_2\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"type\": \"agentFlow\",\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2\"\n    }\n  ]\n}",
      "type": "AGENTFLOW"
    }
  ],
  "AssistantFlow": [],
  "AssistantCustom": [],
  "AssistantOpenAI": [],
  "AssistantAzure": [],
  "ChatFlow": [
    {
      "id": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "name": "Martita_IA_Flow",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"apiLoader_0\",\n      \"position\": {\n        \"x\": -166.343619787935,\n        \"y\": 399.89435338862205\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"apiLoader_0\",\n        \"label\": \"API Loader\",\n        \"version\": 2.1,\n        \"name\": \"apiLoader\",\n        \"type\": \"Document\",\n        \"baseClasses\": [\n          \"Document\"\n        ],\n        \"category\": \"Document Loaders\",\n        \"description\": \"Load data from an API\",\n        \"inputParams\": [\n          {\n            \"label\": \"Method\",\n            \"name\": \"method\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"GET\",\n                \"name\": \"GET\"\n              },\n              {\n                \"label\": \"POST\",\n                \"name\": \"POST\"\n              }\n            ],\n            \"id\": \"apiLoader_0-input-method-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"URL\",\n            \"name\": \"url\",\n            \"type\": \"string\",\n            \"id\": \"apiLoader_0-input-url-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Headers\",\n            \"name\": \"headers\",\n            \"type\": \"json\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-headers-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"SSL Certificate\",\n            \"description\": \"Please upload a SSL certificate file in either .pem or .crt\",\n            \"name\": \"caFile\",\n            \"type\": \"file\",\n            \"fileType\": \".pem, .crt\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-caFile-file\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Body\",\n            \"name\": \"body\",\n            \"type\": \"json\",\n            \"description\": \"JSON body for the POST request. If not specified, agent will try to figure out itself from AIPlugin if provided\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-body-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Additional Metadata\",\n            \"name\": \"metadata\",\n            \"type\": \"json\",\n            \"description\": \"Additional metadata to be added to the extracted documents\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"apiLoader_0-input-metadata-json\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Omit Metadata Keys\",\n            \"name\": \"omitMetadataKeys\",\n            \"type\": \"string\",\n            \"rows\": 4,\n            \"description\": \"Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field\",\n            \"placeholder\": \"key1, key2, key3.nestedKey1\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"apiLoader_0-input-omitMetadataKeys-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Text Splitter\",\n            \"name\": \"textSplitter\",\n            \"type\": \"TextSplitter\",\n            \"optional\": true,\n            \"id\": \"apiLoader_0-input-textSplitter-TextSplitter\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"textSplitter\": \"\",\n          \"method\": \"GET\",\n          \"url\": \"http://backend:8000/construir-tramite/estructurado\",\n          \"headers\": \"\",\n          \"body\": \"\",\n          \"metadata\": \"\",\n          \"omitMetadataKeys\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"Array of document objects containing metadata and pageContent\",\n            \"options\": [\n              {\n                \"id\": \"apiLoader_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"apiLoader_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"document\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"document\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 509,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -166.343619787935,\n        \"y\": 399.89435338862205\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"googleGenerativeAiEmbeddings_0\",\n      \"position\": {\n        \"x\": -177.07919178623737,\n        \"y\": 1054.4384799357392\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"googleGenerativeAiEmbeddings_0\",\n        \"label\": \"GoogleGenerativeAI Embeddings\",\n        \"version\": 2,\n        \"name\": \"googleGenerativeAiEmbeddings\",\n        \"type\": \"GoogleGenerativeAiEmbeddings\",\n        \"baseClasses\": [\n          \"GoogleGenerativeAiEmbeddings\",\n          \"Embeddings\"\n        ],\n        \"category\": \"Embeddings\",\n        \"description\": \"Google Generative API to generate embeddings for a given text\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"embedding-001\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Task Type\",\n            \"name\": \"tasktype\",\n            \"type\": \"options\",\n            \"description\": \"Type of task for which the embedding will be used\",\n            \"options\": [\n              {\n                \"label\": \"TASK_TYPE_UNSPECIFIED\",\n                \"name\": \"TASK_TYPE_UNSPECIFIED\"\n              },\n              {\n                \"label\": \"RETRIEVAL_QUERY\",\n                \"name\": \"RETRIEVAL_QUERY\"\n              },\n              {\n                \"label\": \"RETRIEVAL_DOCUMENT\",\n                \"name\": \"RETRIEVAL_DOCUMENT\"\n              },\n              {\n                \"label\": \"SEMANTIC_SIMILARITY\",\n                \"name\": \"SEMANTIC_SIMILARITY\"\n              },\n              {\n                \"label\": \"CLASSIFICATION\",\n                \"name\": \"CLASSIFICATION\"\n              },\n              {\n                \"label\": \"CLUSTERING\",\n                \"name\": \"CLUSTERING\"\n              }\n            ],\n            \"default\": \"TASK_TYPE_UNSPECIFIED\",\n            \"id\": \"googleGenerativeAiEmbeddings_0-input-tasktype-options\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"modelName\": \"embedding-001\",\n          \"tasktype\": \"TASK_TYPE_UNSPECIFIED\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n            \"name\": \"googleGenerativeAiEmbeddings\",\n            \"label\": \"GoogleGenerativeAiEmbeddings\",\n            \"description\": \"Google Generative API to generate embeddings for a given text\",\n            \"type\": \"GoogleGenerativeAiEmbeddings | Embeddings\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 472,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -177.07919178623737,\n        \"y\": 1054.4384799357392\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"faiss_0\",\n      \"position\": {\n        \"x\": 552.7455578583165,\n        \"y\": 725.7816352501283\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"faiss_0\",\n        \"label\": \"Faiss\",\n        \"version\": 1,\n        \"name\": \"faiss\",\n        \"type\": \"Faiss\",\n        \"baseClasses\": [\n          \"Faiss\",\n          \"VectorStoreRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Vector Stores\",\n        \"description\": \"Upsert embedded data and perform similarity search upon query using Faiss library from Meta\",\n        \"inputParams\": [\n          {\n            \"label\": \"Base Path to load\",\n            \"name\": \"basePath\",\n            \"description\": \"Path to load faiss.index file\",\n            \"placeholder\": \"C:\\\\Users\\\\User\\\\Desktop\",\n            \"type\": \"string\",\n            \"id\": \"faiss_0-input-basePath-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top K\",\n            \"name\": \"topK\",\n            \"description\": \"Number of top results to fetch. Default to 4\",\n            \"placeholder\": \"4\",\n            \"type\": \"number\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-topK-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Document\",\n            \"name\": \"document\",\n            \"type\": \"Document\",\n            \"list\": true,\n            \"optional\": true,\n            \"id\": \"faiss_0-input-document-Document\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Embeddings\",\n            \"name\": \"embeddings\",\n            \"type\": \"Embeddings\",\n            \"id\": \"faiss_0-input-embeddings-Embeddings\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"document\": [\n            \"{{apiLoader_0.data.instance}}\"\n          ],\n          \"embeddings\": \"{{googleGenerativeAiEmbeddings_0.data.instance}}\",\n          \"basePath\": \"/root/.flowise/embeddings/\",\n          \"topK\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"Faiss Retriever\",\n                \"description\": \"\",\n                \"type\": \"Faiss | VectorStoreRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"faiss_0-output-vectorStore-Faiss|SaveableVectorStore|VectorStore\",\n                \"name\": \"vectorStore\",\n                \"label\": \"Faiss Vector Store\",\n                \"description\": \"\",\n                \"type\": \"Faiss | SaveableVectorStore | VectorStore\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 464,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 552.7455578583165,\n        \"y\": 725.7816352501283\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_0\",\n      \"position\": {\n        \"x\": 1063.2360426708017,\n        \"y\": -80.25597234853285\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_0\",\n        \"label\": \"ChatGoogleGenerativeAI\",\n        \"version\": 3.1,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_0-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-customModelName-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-temperature-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-streaming-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-maxOutputTokens-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topP-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-topK-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Safety Settings\",\n            \"name\": \"safetySettings\",\n            \"type\": \"array\",\n            \"description\": \"Safety settings for the model. Refer to the <a href=\\\"https://ai.google.dev/gemini-api/docs/safety-settings\\\">official guide</a> on how to use Safety Settings\",\n            \"array\": [\n              {\n                \"label\": \"Harm Category\",\n                \"name\": \"harmCategory\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"Dangerous\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Promotes, facilitates, or encourages harmful acts.\"\n                  },\n                  {\n                    \"label\": \"Harassment\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Negative or harmful comments targeting identity and/or protected attributes.\"\n                  },\n                  {\n                    \"label\": \"Hate Speech\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Content that is rude, disrespectful, or profane.\"\n                  },\n                  {\n                    \"label\": \"Sexually Explicit\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Contains references to sexual acts or other lewd content.\"\n                  },\n                  {\n                    \"label\": \"Civic Integrity\",\n                    \"name\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n                    \"description\": \"Election-related queries.\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Harm Block Threshold\",\n                \"name\": \"harmBlockThreshold\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"None\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"Always show regardless of probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Only High\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Block when high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Medium and Above\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Block when medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Low and Above\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Block when low, medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Threshold Unspecified (Default Threshold)\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified, block using default threshold\"\n                  }\n                ]\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-safetySettings-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"description\": \"Base URL for the API. Leave empty to use the default.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-baseUrl-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-allowImageUploads-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"{{inMemoryCache_0.data.instance}}\",\n          \"modelName\": \"gemini-2.0-flash\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.2\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"safetySettings\": \"\",\n          \"baseUrl\": \"\",\n          \"allowImageUploads\": false\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 675,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1063.2360426708017,\n        \"y\": -80.25597234853285\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmFilterRetriever_0\",\n      \"position\": {\n        \"x\": 1071.503095056429,\n        \"y\": 820.8527376848419\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"llmFilterRetriever_0\",\n        \"label\": \"LLM Filter Retriever\",\n        \"version\": 1,\n        \"name\": \"llmFilterRetriever\",\n        \"type\": \"LLMFilterRetriever\",\n        \"baseClasses\": [\n          \"LLMFilterRetriever\",\n          \"BaseRetriever\"\n        ],\n        \"category\": \"Retrievers\",\n        \"description\": \"Iterate over the initially returned documents and extract, from each, only the content that is relevant to the query\",\n        \"inputParams\": [\n          {\n            \"label\": \"Query\",\n            \"name\": \"query\",\n            \"type\": \"string\",\n            \"description\": \"Query to retrieve documents from retriever. If not specified, user question will be used\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"id\": \"llmFilterRetriever_0-input-query-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"baseRetriever\",\n            \"type\": \"VectorStoreRetriever\",\n            \"id\": \"llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Language Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseLanguageModel\",\n            \"id\": \"llmFilterRetriever_0-input-model-BaseLanguageModel\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"baseRetriever\": \"{{faiss_0.data.instance}}\",\n          \"model\": \"{{chatGoogleGenerativeAI_1.data.instance}}\",\n          \"query\": \"{x}\"\n        },\n        \"outputAnchors\": [\n          {\n            \"name\": \"output\",\n            \"label\": \"Output\",\n            \"type\": \"options\",\n            \"description\": \"\",\n            \"options\": [\n              {\n                \"id\": \"llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever\",\n                \"name\": \"retriever\",\n                \"label\": \"LLM Filter Retriever\",\n                \"description\": \"\",\n                \"type\": \"LLMFilterRetriever | BaseRetriever\"\n              },\n              {\n                \"id\": \"llmFilterRetriever_0-output-document-Document|json\",\n                \"name\": \"document\",\n                \"label\": \"Document\",\n                \"description\": \"Array of document objects containing metadata and pageContent\",\n                \"type\": \"Document | json\"\n              },\n              {\n                \"id\": \"llmFilterRetriever_0-output-text-string|json\",\n                \"name\": \"text\",\n                \"label\": \"Text\",\n                \"description\": \"Concatenated string from pageContent of documents\",\n                \"type\": \"string | json\"\n              }\n            ],\n            \"default\": \"retriever\"\n          }\n        ],\n        \"outputs\": {\n          \"output\": \"retriever\"\n        },\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 413,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1071.503095056429,\n        \"y\": 820.8527376848419\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conversationalRetrievalQAChain_0\",\n      \"position\": {\n        \"x\": 1671.1273723742863,\n        \"y\": 795.9535459456622\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"conversationalRetrievalQAChain_0\",\n        \"label\": \"Conversational Retrieval QA Chain\",\n        \"version\": 3,\n        \"name\": \"conversationalRetrievalQAChain\",\n        \"type\": \"ConversationalRetrievalQAChain\",\n        \"baseClasses\": [\n          \"ConversationalRetrievalQAChain\",\n          \"BaseChain\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chains\",\n        \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n        \"inputParams\": [\n          {\n            \"label\": \"Return Source Documents\",\n            \"name\": \"returnSourceDocuments\",\n            \"type\": \"boolean\",\n            \"optional\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-returnSourceDocuments-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Rephrase Prompt\",\n            \"name\": \"rephrasePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Using previous chat history, rephrase question into a standalone question\",\n            \"warning\": \"Prompt must include input variables: {chat_history} and {question}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone Question:\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-rephrasePrompt-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Response Prompt\",\n            \"name\": \"responsePrompt\",\n            \"type\": \"string\",\n            \"description\": \"Taking the rephrased question, search for answer from the provided context\",\n            \"warning\": \"Prompt must include input variable: {context}\",\n            \"rows\": 4,\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"default\": \"I want you to act as a document that I am having a conversation with. Your name is \\\"AI Assistant\\\". Using the provided context, answer the user's question to the best of your ability using the resources provided.\\nIf there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure\\\" and stop after that. Refuse to answer any question not about the info. Never break character.\\n------------\\n{context}\\n------------\\nREMEMBER: If there is no relevant information within the context, just say \\\"Hmm, I'm not sure\\\". Don't try to make up an answer. Never break character.\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-responsePrompt-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Chat Model\",\n            \"name\": \"model\",\n            \"type\": \"BaseChatModel\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Vector Store Retriever\",\n            \"name\": \"vectorStoreRetriever\",\n            \"type\": \"BaseRetriever\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory\",\n            \"name\": \"memory\",\n            \"type\": \"BaseMemory\",\n            \"optional\": true,\n            \"description\": \"If left empty, a default BufferMemory will be used\",\n            \"id\": \"conversationalRetrievalQAChain_0-input-memory-BaseMemory\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input Moderation\",\n            \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\",\n            \"name\": \"inputModeration\",\n            \"type\": \"Moderation\",\n            \"optional\": true,\n            \"list\": true,\n            \"id\": \"conversationalRetrievalQAChain_0-input-inputModeration-Moderation\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"model\": \"{{chatGoogleGenerativeAI_0.data.instance}}\",\n          \"vectorStoreRetriever\": \"{{llmFilterRetriever_0.data.instance}}\",\n          \"memory\": \"\",\n          \"returnSourceDocuments\": true,\n          \"rephrasePrompt\": \"INSTRUCCIONES:\\n\\nPrimer mensaje:\\nIdentifica el trámite que más se aproxime a la consulta del usuario y preséntalo. Pregunta si desea más información sobre cómo realizar el proceso.\\n\\nSegundo mensaje:\\nProporciona más detalles sobre el trámite y los requisitos necesarios para llevarlo a cabo.\\n\\nTercer mensaje:\\nExplica los pasos a seguir para completar el trámite. Incluye los enlaces a los formularios u otros documentos necesarios.\\n\\nMensaje final:\\nPregunta si desea ayuda con otro trámite. Agradece su consulta y sugiérele que deje un \\\"like\\\" o una \\\"manito arriba\\\" si la información fue útil.\\n\\nHistorial del Chat:\\n{chat_history}\\n\\nPregunta de Seguimiento:{question}\\n\\nPregunta Independiente:\",\n          \"responsePrompt\": \"Eres Martita AI, una asesora técnica municipal del GAD de Cayambe. \\n\\n\\n\\nREGLAS INQUEBRANTABLES:\\n- NUNCA inventes información que no esté en el contexto\\n- Siempre responde en español\\n- Mantén un tono profesional pero amigable\\n- Si hay dudas, deriva al contacto humano correspondiente\\n\\nContexto Proporcionado:\\n------------\\n{context}\\n------------\\n\\nPregunta del Usuario: {question}\\nINSTRUCCIONES:\\n\\nPrimer mensaje:\\nIdentifica el trámite que más se aproxime a la consulta del usuario y preséntalo. Pregunta si desea más información sobre cómo realizar el proceso.\\n\\nSegundo mensaje:\\nProporciona más detalles sobre el trámite y los requisitos necesarios para llevarlo a cabo.\\n\\nTercer mensaje:\\nExplica los pasos a seguir para completar el trámite. Incluye los enlaces a los formularios u otros documentos necesarios.\\n\\nMensaje final:\\nPregunta si desea ayuda con otro trámite. Agradece su consulta y sugiérele que deje un \\\"like\\\" o una \\\"manito arriba\\\" si la información fue útil.\",\n          \"inputModeration\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conversationalRetrievalQAChain_0-output-conversationalRetrievalQAChain-ConversationalRetrievalQAChain|BaseChain|Runnable\",\n            \"name\": \"conversationalRetrievalQAChain\",\n            \"label\": \"ConversationalRetrievalQAChain\",\n            \"description\": \"Document QA - built on RetrievalQAChain to provide a chat history component\",\n            \"type\": \"ConversationalRetrievalQAChain | BaseChain | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 536,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1671.1273723742863,\n        \"y\": 795.9535459456622\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"chatGoogleGenerativeAI_1\",\n      \"position\": {\n        \"x\": 569.4321312826156,\n        \"y\": 1342.7945064455434\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"chatGoogleGenerativeAI_1\",\n        \"label\": \"ChatGoogleGenerativeAI (1)\",\n        \"version\": 3.1,\n        \"name\": \"chatGoogleGenerativeAI\",\n        \"type\": \"ChatGoogleGenerativeAI\",\n        \"baseClasses\": [\n          \"ChatGoogleGenerativeAI\",\n          \"LangchainChatGoogleGenerativeAI\",\n          \"BaseChatModel\",\n          \"BaseLanguageModel\",\n          \"Runnable\"\n        ],\n        \"category\": \"Chat Models\",\n        \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n        \"inputParams\": [\n          {\n            \"label\": \"Connect Credential\",\n            \"name\": \"credential\",\n            \"type\": \"credential\",\n            \"credentialNames\": [\n              \"googleGenerativeAI\"\n            ],\n            \"optional\": false,\n            \"description\": \"Google Generative AI credential.\",\n            \"id\": \"chatGoogleGenerativeAI_1-input-credential-credential\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Model Name\",\n            \"name\": \"modelName\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"default\": \"gemini-1.5-flash-latest\",\n            \"id\": \"chatGoogleGenerativeAI_1-input-modelName-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Custom Model Name\",\n            \"name\": \"customModelName\",\n            \"type\": \"string\",\n            \"placeholder\": \"gemini-1.5-pro-exp-0801\",\n            \"description\": \"Custom model name to use. If provided, it will override the model selected\",\n            \"additionalParams\": true,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-customModelName-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Temperature\",\n            \"name\": \"temperature\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"default\": 0.9,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-temperature-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Streaming\",\n            \"name\": \"streaming\",\n            \"type\": \"boolean\",\n            \"default\": true,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-streaming-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Output Tokens\",\n            \"name\": \"maxOutputTokens\",\n            \"type\": \"number\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-maxOutputTokens-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Probability\",\n            \"name\": \"topP\",\n            \"type\": \"number\",\n            \"step\": 0.1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-topP-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Top Next Highest Probability Tokens\",\n            \"name\": \"topK\",\n            \"type\": \"number\",\n            \"description\": \"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive\",\n            \"step\": 1,\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-topK-number\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Safety Settings\",\n            \"name\": \"safetySettings\",\n            \"type\": \"array\",\n            \"description\": \"Safety settings for the model. Refer to the <a href=\\\"https://ai.google.dev/gemini-api/docs/safety-settings\\\">official guide</a> on how to use Safety Settings\",\n            \"array\": [\n              {\n                \"label\": \"Harm Category\",\n                \"name\": \"harmCategory\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"Dangerous\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Promotes, facilitates, or encourages harmful acts.\"\n                  },\n                  {\n                    \"label\": \"Harassment\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Negative or harmful comments targeting identity and/or protected attributes.\"\n                  },\n                  {\n                    \"label\": \"Hate Speech\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Content that is rude, disrespectful, or profane.\"\n                  },\n                  {\n                    \"label\": \"Sexually Explicit\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Contains references to sexual acts or other lewd content.\"\n                  },\n                  {\n                    \"label\": \"Civic Integrity\",\n                    \"name\": \"HARM_CATEGORY_CIVIC_INTEGRITY\",\n                    \"description\": \"Election-related queries.\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Harm Block Threshold\",\n                \"name\": \"harmBlockThreshold\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"None\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"Always show regardless of probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Only High\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Block when high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Medium and Above\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Block when medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Low and Above\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Block when low, medium or high probability of unsafe content\"\n                  },\n                  {\n                    \"label\": \"Threshold Unspecified (Default Threshold)\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified, block using default threshold\"\n                  }\n                ]\n              }\n            ],\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-safetySettings-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Base URL\",\n            \"name\": \"baseUrl\",\n            \"type\": \"string\",\n            \"description\": \"Base URL for the API. Leave empty to use the default.\",\n            \"optional\": true,\n            \"additionalParams\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-baseUrl-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Allow Image Uploads\",\n            \"name\": \"allowImageUploads\",\n            \"type\": \"boolean\",\n            \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\",\n            \"default\": false,\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-allowImageUploads-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [\n          {\n            \"label\": \"Cache\",\n            \"name\": \"cache\",\n            \"type\": \"BaseCache\",\n            \"optional\": true,\n            \"id\": \"chatGoogleGenerativeAI_1-input-cache-BaseCache\",\n            \"display\": true\n          }\n        ],\n        \"inputs\": {\n          \"cache\": \"\",\n          \"modelName\": \"gemini-2.0-flash\",\n          \"customModelName\": \"\",\n          \"temperature\": \"0.1\",\n          \"streaming\": true,\n          \"maxOutputTokens\": \"\",\n          \"topP\": \"\",\n          \"topK\": \"\",\n          \"safetySettings\": \"\",\n          \"baseUrl\": \"\",\n          \"allowImageUploads\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n            \"name\": \"chatGoogleGenerativeAI\",\n            \"label\": \"ChatGoogleGenerativeAI\",\n            \"description\": \"Wrapper around Google Gemini large language models that use the Chat endpoint\",\n            \"type\": \"ChatGoogleGenerativeAI | LangchainChatGoogleGenerativeAI | BaseChatModel | BaseLanguageModel | Runnable\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 675,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 569.4321312826156,\n        \"y\": 1342.7945064455434\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"inMemoryCache_0\",\n      \"position\": {\n        \"x\": 559.1182205060782,\n        \"y\": -2.9027537712468927\n      },\n      \"type\": \"customNode\",\n      \"data\": {\n        \"id\": \"inMemoryCache_0\",\n        \"label\": \"InMemory Cache\",\n        \"version\": 1,\n        \"name\": \"inMemoryCache\",\n        \"type\": \"InMemoryCache\",\n        \"baseClasses\": [\n          \"InMemoryCache\",\n          \"BaseCache\"\n        ],\n        \"category\": \"Cache\",\n        \"description\": \"Cache LLM response in memory, will be cleared once app restarted\",\n        \"inputParams\": [],\n        \"inputAnchors\": [],\n        \"inputs\": {},\n        \"outputAnchors\": [\n          {\n            \"id\": \"inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache\",\n            \"name\": \"inMemoryCache\",\n            \"label\": \"InMemoryCache\",\n            \"description\": \"Cache LLM response in memory, will be cleared once app restarted\",\n            \"type\": \"InMemoryCache | BaseCache\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 300,\n      \"height\": 149,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 559.1182205060782,\n        \"y\": -2.9027537712468927\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"chatGoogleGenerativeAI_0\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-model-BaseChatModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-model-BaseChatModel\"\n    },\n    {\n      \"source\": \"llmFilterRetriever_0\",\n      \"sourceHandle\": \"llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever\",\n      \"target\": \"conversationalRetrievalQAChain_0\",\n      \"targetHandle\": \"conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"llmFilterRetriever_0-llmFilterRetriever_0-output-retriever-LLMFilterRetriever|BaseRetriever-conversationalRetrievalQAChain_0-conversationalRetrievalQAChain_0-input-vectorStoreRetriever-BaseRetriever\"\n    },\n    {\n      \"source\": \"faiss_0\",\n      \"sourceHandle\": \"faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever\",\n      \"target\": \"llmFilterRetriever_0\",\n      \"targetHandle\": \"llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\",\n      \"type\": \"buttonedge\",\n      \"id\": \"faiss_0-faiss_0-output-retriever-Faiss|VectorStoreRetriever|BaseRetriever-llmFilterRetriever_0-llmFilterRetriever_0-input-baseRetriever-VectorStoreRetriever\"\n    },\n    {\n      \"source\": \"chatGoogleGenerativeAI_1\",\n      \"sourceHandle\": \"chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable\",\n      \"target\": \"llmFilterRetriever_0\",\n      \"targetHandle\": \"llmFilterRetriever_0-input-model-BaseLanguageModel\",\n      \"type\": \"buttonedge\",\n      \"id\": \"chatGoogleGenerativeAI_1-chatGoogleGenerativeAI_1-output-chatGoogleGenerativeAI-ChatGoogleGenerativeAI|LangchainChatGoogleGenerativeAI|BaseChatModel|BaseLanguageModel|Runnable-llmFilterRetriever_0-llmFilterRetriever_0-input-model-BaseLanguageModel\"\n    },\n    {\n      \"source\": \"apiLoader_0\",\n      \"sourceHandle\": \"apiLoader_0-output-document-Document|json\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-document-Document\",\n      \"type\": \"buttonedge\",\n      \"id\": \"apiLoader_0-apiLoader_0-output-document-Document|json-faiss_0-faiss_0-input-document-Document\"\n    },\n    {\n      \"source\": \"googleGenerativeAiEmbeddings_0\",\n      \"sourceHandle\": \"googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings\",\n      \"target\": \"faiss_0\",\n      \"targetHandle\": \"faiss_0-input-embeddings-Embeddings\",\n      \"type\": \"buttonedge\",\n      \"id\": \"googleGenerativeAiEmbeddings_0-googleGenerativeAiEmbeddings_0-output-googleGenerativeAiEmbeddings-GoogleGenerativeAiEmbeddings|Embeddings-faiss_0-faiss_0-input-embeddings-Embeddings\"\n    },\n    {\n      \"source\": \"inMemoryCache_0\",\n      \"sourceHandle\": \"inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache\",\n      \"target\": \"chatGoogleGenerativeAI_0\",\n      \"targetHandle\": \"chatGoogleGenerativeAI_0-input-cache-BaseCache\",\n      \"type\": \"buttonedge\",\n      \"id\": \"inMemoryCache_0-inMemoryCache_0-output-inMemoryCache-InMemoryCache|BaseCache-chatGoogleGenerativeAI_0-chatGoogleGenerativeAI_0-input-cache-BaseCache\"\n    }\n  ]\n}",
      "type": "CHATFLOW"
    }
  ],
  "ChatMessage": [
    {
      "id": "ad1b4277-4e89-433b-93e8-14c08359cada",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "hola",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:50:42.557Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "2768bb66-5677-46c8-af3b-2282cd82a38d",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "¡Hola! 👋 Soy Martita AI, tu asesora técnica municipal del GAD de Cayambe.\n\nParece que necesitas información sobre algún trámite. Para ayudarte mejor, ¿podrías decirme qué tipo de trámite te interesa realizar? Así podré darte la información precisa que necesitas. 😊\n",
      "sourceDocuments": "[]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:50:46.372Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "61437893-9177-4787-9031-c0843aba84de",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "necesito cambiar el propietario de un terreno , como puedo hacer el tramite",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:43.853Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "d5e892b9-4ba4-4226-8201-9620eb565be9",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "¡Entiendo! El trámite que necesitas es la **Transferencia de Dominio de un bien inmueble**. ¿Te gustaría que te explique cómo realizar este proceso aquí en Cayambe? 😊\n",
      "sourceDocuments": "[]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:49.162Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "6f6bf4ce-faa7-46ed-bb04-f8907b1c5e65",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "si ",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:51:59.092Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "d0427f5f-ab18-4497-9002-59f017e07437",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "¡Perfecto! Para realizar la **Emisión de Avalúo para Transferencia de Dominio Total** en Cayambe, necesitarás los siguientes requisitos:\n\n1.  Original de la cédula de ciudadanía del vendedor y comprador.\n2.  Copia de la escritura del terreno.\n3.  Pago del impuesto predial actualizado.\n4.  Certificado de gravámenes actualizado.\n5.  Formulario de Trámites de Avalúos y Catastros debidamente llenado.\n6.  Certificado de plusvalía (solo si el terreno es urbano).\n\n¿Tienes alguna pregunta sobre estos requisitos?\n",
      "sourceDocuments": "[{\"pageContent\":\"- \\\"nombre\\\": \\\"EMISIÃ“N DE AVALÃšO PARA TRANSFERENCIA DE DOMINIO TOTAL\\\",\\n- \\\"descripcion\\\": \\\"Determinar y emitir el avalÃºo para la transferencia total de\\\\ndominio de los predios.\\\",\\n- \\\"contexto\\\": \\\"Determinar y emitir el avalÃºo para la transferencia total de dominio de los predios.\\\",\\n- \\\"requisitos\\\": [\\n      {\\n        \\\"contexto\\\": \\\"Cualquier persona\\\",\\n        \\\"requisito\\\": \\\"1. Original de cÃ©dula de ciudadanÃ­a de vendedor y\\\\ncomprador.\\\\n2. Copia de la escritura.\\\\n3. Pago del impuesto predial actualizado.\\\\n4. Certificado de gravÃ¡menes actualizado.\\\\n5. Formulario de TrÃ¡mites de AvalÃºos y Catastros.\\\\n6. Certificado de plusvalÃ­a (sÃ³lo urbano).\\\",\\n      }\\n    ],\\n- \\\"formularios\\\": [\\n      {\\n        \\\"url\\\": \\\"https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf\\\",\\n        \\\"contexto\\\": \\\"FORMULARIO DE TRÃMITES DE AVALÃšOS Y CATASTROS\\\",\\n      }\\n    ],\\n- \\\"pasos\\\": [\\n      {\\n        \\\"contexto\\\": \\\"Cualquier persona\\\",\\n        \\\"paso\\\": \\\"1. Imprimir y llenar el Formulario de TrÃ¡mites de AvalÃºos y\\\\nCatastros que estÃ¡ disponible en la pÃ¡gina web del GADIPMC. El formulario tambiÃ©n puede ser completado desde el\\\\ncomputador para luego imprimirse.\\\\n2. Adquirir certificado de plusvalÃ­a en caja del GADIP-MC.\\\\n3. Ingresar requisitos en la SecretarÃ­a de la DirecciÃ³n de\\\\nAvalÃºos y Catastros.\\\",\\n      }\\n    ],\\n- \\\"nombre\\\": \\\"DirecciÃ³n de AvalÃºos y Catastros\\\",\\n- \\\"descripcion\\\": \\\"Encargada del levantamiento catastral, valoraciÃ³n de predios urbanos y rurales, actualizaciÃ³n de la base catastral, emisiÃ³n de certificados de avalÃºos y mantenimiento del sistema de informaciÃ³n territorial\\\",\",\"metadata\":{\"url\":\"http://backend:8000/construir-tramite/estructurado\"}}]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:52:09.399Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "11da8d2e-cc31-421e-839c-27ac37a99e5f",
      "role": "userMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "cuentame mas",
      "sourceDocuments": null,
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:53:26.257Z",
      "leadEmail": null,
      "followUpPrompts": null
    },
    {
      "id": "c69a17a6-e014-4b87-bb5b-e70ba56fd7aa",
      "role": "apiMessage",
      "chatflowid": "34f30a9d-07fc-4a04-9c4c-84be79997556",
      "executionId": null,
      "content": "¡Claro! Aquí están los pasos a seguir para completar el trámite:\n\n1.  **Descarga y llena el Formulario de Trámites de Avalúos y Catastros:** Puedes encontrarlo en la página web del GADIPMC: [https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf](https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf). Puedes completar el formulario directamente en tu computadora antes de imprimirlo si lo prefieres.\n2.  **Adquiere el certificado de plusvalía:** Si tu terreno es urbano, debes obtener este certificado en las cajas del GADIP-MC.\n3.  **Entrega los requisitos:** Una vez que tengas todos los documentos y el formulario lleno, debes entregarlos en la Secretaría de la Dirección de Avalúos y Catastros.\n\n¡Y eso es todo! ¿Te puedo ayudar con algo más? 😊\n",
      "sourceDocuments": "[{\"pageContent\":\"- `nombre\\\": \\\"EMISIÃ“N DE AVALÃšO PARA TRANSFERENCIA DE DOMINIO TOTAL`\\n- `descripcion\\\": \\\"Determinar y emitir el avalÃºo para la transferencia total de\\\\ndominio de los predios.`\\n- `requisito\\\": \\\"1. Original de cÃ©dula de ciudadanÃ­a de vendedor y\\\\ncomprador.\\\\n2. Copia de la escritura.\\\\n3. Pago del impuesto predial actualizado.\\\\n4. Certificado de gravÃ¡menes actualizado.\\\\n5. Formulario de TrÃ¡mites de AvalÃºos y Catastros.\\\\n6. Certificado de plusvalÃ­a (sÃ³lo urbano).`\\n- `url\\\": \\\"https://municipiocayambe.gob.ec/images/gtramites/FAC.pdf`\\n- `contexto\\\": \\\"FORMULARIO DE TRÃMITES DE AVALÃšOS Y CATASTROS`\\n- `paso\\\": \\\"1. Imprimir y llenar el Formulario de TrÃ¡mites de AvalÃºos y\\\\nCatastros que estÃ¡ disponible en la pÃ¡gina web del GADIPMC. El formulario tambiÃ©n puede ser completado desde el\\\\ncomputador para luego imprimirse.\\\\n2. Adquirir certificado de plusvalÃ­a en caja del GADIP-MC.\\\\n3. Ingresar requisitos en la SecretarÃ­a de la DirecciÃ³n de\\\\nAvalÃºos y Catastros.`\\n- `nombre\\\": \\\"DirecciÃ³n de AvalÃºos y Catastros`\",\"metadata\":{\"url\":\"http://backend:8000/construir-tramite/estructurado\"}}]",
      "usedTools": null,
      "fileAnnotations": null,
      "agentReasoning": null,
      "fileUploads": null,
      "artifacts": null,
      "action": null,
      "chatType": "INTERNAL",
      "chatId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "memoryType": "",
      "sessionId": "887ca2b4-b2bf-48ac-9323-0c758787eeca",
      "createdDate": "2025-08-11T22:53:35.132Z",
      "leadEmail": null,
      "followUpPrompts": null
    }
  ],
  "ChatMessageFeedback": [],
  "CustomTemplate": [],
  "DocumentStore": [],
  "DocumentStoreFileChunk": [],
  "Execution": [],
  "Tool": [],
  "Variable": []
}